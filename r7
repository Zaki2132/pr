body = {
    "filters": [],
    "page": 0,
    "size": 100
}

assets = []

while True:
    response = requests.post(url, headers=headers, json=body)
    if response.status_code != 200:
        print("Erreur:", response.status_code, response.text)
        break

    data = response.json()
    assets.extend(data.get("resources", []))

    if data.get("page", 0) >= data.get("totalPages", 1) - 1:
        break

    body["page"] += 1

print(f"Total assets récupérés : {len(assets)}")



import csv

# Step 1: collect all unique keys across all assets
all_keys = set()
for asset in assets:
    all_keys.update(asset.keys())

# Step 2: write all data to CSV using all discovered keys
with open("assets_full.csv", "w", newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
    writer.writeheader()

    for asset in assets:
        writer.writerow(asset)

print("✅ Exported all assets to assets_full.csv with all fields.")

